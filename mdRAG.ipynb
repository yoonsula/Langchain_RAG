{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "376eab84-889c-4872-bdde-20b0b0fe02f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import DirectoryLoader, UnstructuredMarkdownLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings, OllamaEmbeddings\n",
    "from langchain.vectorstores import utils as chromautils\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import os\n",
    "import shutil\n",
    "import chromadb\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-********************************\"\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = 'hf_********************************'\n",
    "\n",
    "DATA_PATH = 'markdown/'\n",
    "CHROMA_DB_PATH = './db/chromadb/'\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"<|SYSTEM|>\n",
    "            The user is a researcher who wants to set up a synthetic biology experiment.\n",
    "            The assistant is an expert on DNA assembly.\n",
    "            The assistant have documents that describe experiments; Heading level 2 following two hashes(##) is the top heading in a document, which means the name of an experiment task consisting of many unit processes.\n",
    "            If given information of the target experiment, the name of the target experiment will be a level-2 heading and the assistant needs to explain its experimental steps which are called \"unit process\" for each.\n",
    "            The assistant should specify in English the device or materials(including their volume) which should be used for the experiment.\n",
    "            When the assistant writes the experimental steps(that is unit process) for planning an experiment, it should follow the format of the documentation provided;\n",
    "            Heading level 3 with three hashes(###) should be used as each unit process title with no indices other than the hashes such as \"Step 1\", \"1.\" and so on.\n",
    "            The title of the unit process should be followed by the details of the unit process including \"Material\", \"Equipment\" and its \"Method\" which should be with 4 hashes(####). \"Material\", \"Equipment\" and \"Method\" as level-4 headings should be written in English.\n",
    "            More than 4 hashes or no hashes should be used for describing the details of the unit process.\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "{system}\n",
    "\n",
    "Extract the relevant content and hierarchy based on the following context.:\n",
    "{context}\n",
    "---\n",
    "\n",
    "Answer the part of the question based on the above context: {question}\n",
    "You must answer in a structured format separated by headers, such as above context.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27229165-a351-4067-9ddd-26ab1ddb5550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Markdown files.\n",
    "def load_documents():\n",
    "    loader = DirectoryLoader(DATA_PATH, \n",
    "                             glob=\"*.md\", \n",
    "                             show_progress=True, \n",
    "                             loader_cls=TextLoader)\n",
    "    documents = loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d3447d6-4877-49e2-94fc-79c4047e1e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 2630.89it/s]\n"
     ]
    }
   ],
   "source": [
    "documents = load_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9feff308-279f-4385-9ce6-bbb8ff5f3ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(documents: list[Document]):\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\")\n",
    "    ]\n",
    "    \n",
    "    text_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=headers_to_split_on,\n",
    "        strip_headers = False\n",
    "    )\n",
    "    # print(documents[0])\n",
    "\n",
    "    chunks = []\n",
    "    for document in documents:\n",
    "        chunk = text_splitter.split_text(document.page_content)\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    # text_splitter = CharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "    # chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63686bcf-c106-4f8d-9080-5da8ade01f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = split_text(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9f777b8-a12e-4fb4-aff3-2812bd89a9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name='jhgan/ko-sroberta-nli',\n",
    "    model_kwargs={'device':'cuda'},\n",
    "    encode_kwargs={'normalize_embeddings':True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42be31d8-18a7-46df-a515-dec3c1ec20d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents Loaded: 189\n"
     ]
    }
   ],
   "source": [
    "# Connect to the ChromaDB client\n",
    "persist_dir = CHROMA_DB_PATH\n",
    "client = chromadb.PersistentClient(path=CHROMA_DB_PATH)\n",
    "\n",
    "collection = client.get_or_create_collection(name='labnote')\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=chunks[0], # 여러 파일 올리는 방법 확인 필요\n",
    "    embedding=embeddings_model,\n",
    "    client=client,\n",
    "    collection_name='labnote_4',\n",
    "    persist_directory=persist_dir\n",
    ")\n",
    "\n",
    "vectordb.add_documents(documents=chunks[1])\n",
    "vectordb.add_documents(documents=chunks[2])\n",
    "vectordb.add_documents(documents=chunks[3])\n",
    "\n",
    "print(f\"Documents Loaded: {vectordb._collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51f2748b-5d91-40b8-9704-b93d8f32331e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================\n",
      "4\n",
      "### \\[Liquid handling\\] Golden Gate assembly mixture 제작  \n",
      "#### 20240604  \n",
      "#### 시약  \n",
      "-   DNA parts (spacer), lycopene 들어간 vector (V6L, V7L)  \n",
      "-   DW  \n",
      "-   T4 DNA ligase (HC) (Promega)  \n",
      "-   BsaI restriction enzyme (NEB)  \n",
      "-   10x T4 DNA ligase buffer (Promega)  \n",
      "#### 소모품  \n",
      "-   Pipet tip (10p, 200p tip)  \n",
      "-   PCR tube  \n",
      "-   PCR tube rack  \n",
      "#### 장비  \n",
      "-   Pipet (10p, 2.5p, 200p)  \n",
      "-   freezer  \n",
      "#### 방법  \n",
      "-   대량 자동화 수행 시, Janus, Echo 525 를 위한 추가적인 프로토콜이 필요함\n",
      "-   농도가 측정된 파트들을 계산하여 10 nM (100 fmol/10 $\\mu l$ ) 이상이 되도록 함 (volume은 10 $\\mu l$ 로 맞춤)\n",
      "-   볼륨을 맞추기 위해 DW를 넣어줌\n",
      "-   10x ligase buffer와 ligase, restriction enzyme을 넣음\n",
      "-   많은 양을 제작할 때는 stock으로 만든 후 소분\n",
      "-   실험에 사용된 부품과 양은 Assembly_025.xlsx의 240604 sheet를 참고  \n",
      "![](images/paste-1.png)  \n",
      "#### 결과물  \n",
      "-   Golden Gate assembly를 위한 vector assembly mixture 2 종\n",
      "### \\[Liquid handling\\] Golden Gate assembly mixture 제작  \n",
      "#### 20240604  \n",
      "#### 시약  \n",
      "-   DNA parts (spacer), lycopene 들어간 vector (V6L, V7L)  \n",
      "-   DW  \n",
      "-   T4 DNA ligase (HC) (Promega)  \n",
      "-   BsaI restriction enzyme (NEB)  \n",
      "-   10x T4 DNA ligase buffer (Promega)  \n",
      "#### 소모품  \n",
      "-   Pipet tip (10p, 200p tip)  \n",
      "-   PCR tube  \n",
      "-   PCR tube rack  \n",
      "#### 장비  \n",
      "-   Pipet (10p, 2.5p, 200p)  \n",
      "-   freezer  \n",
      "#### 방법  \n",
      "-   대량 자동화 수행 시, Janus, Echo 525 를 위한 추가적인 프로토콜이 필요함\n",
      "-   농도가 측정된 파트들을 계산하여 10 nM (100 fmol/10 $\\mu l$ ) 이상이 되도록 함 (volume은 10 $\\mu l$ 로 맞춤)\n",
      "-   볼륨을 맞추기 위해 DW를 넣어줌\n",
      "-   10x ligase buffer와 ligase, restriction enzyme을 넣음\n",
      "-   많은 양을 제작할 때는 stock으로 만든 후 소분\n",
      "-   실험에 사용된 부품과 양은 Assembly_025.xlsx의 240604 sheet를 참고  \n",
      "![](images/paste-1.png)  \n",
      "#### 결과물  \n",
      "-   Golden Gate assembly를 위한 vector assembly mixture 2 종\n",
      "====================================\n",
      "10\n",
      "### \\[Liquid handling\\] Golden Gate assembly mixture 제작  \n",
      "#### 20240604  \n",
      "#### 시약  \n",
      "-   DNA parts (spacer), lycopene 들어간 vector (V6L, V7L)  \n",
      "-   DW  \n",
      "-   T4 DNA ligase (HC) (Promega)  \n",
      "-   BsaI restriction enzyme (NEB)  \n",
      "-   10x T4 DNA ligase buffer (Promega)  \n",
      "#### 소모품  \n",
      "-   Pipet tip (10p, 200p tip)  \n",
      "-   PCR tube  \n",
      "-   PCR tube rack  \n",
      "#### 장비  \n",
      "-   Pipet (10p, 2.5p, 200p)  \n",
      "-   freezer  \n",
      "#### 방법  \n",
      "-   대량 자동화 수행 시, Janus, Echo 525 를 위한 추가적인 프로토콜이 필요함\n",
      "-   농도가 측정된 파트들을 계산하여 10 nM (100 fmol/10 $\\mu l$ ) 이상이 되도록 함 (volume은 10 $\\mu l$ 로 맞춤)\n",
      "-   볼륨을 맞추기 위해 DW를 넣어줌\n",
      "-   10x ligase buffer와 ligase, restriction enzyme을 넣음\n",
      "-   많은 양을 제작할 때는 stock으로 만든 후 소분\n",
      "-   실험에 사용된 부품과 양은 Assembly_025.xlsx의 240604 sheet를 참고  \n",
      "![](images/paste-1.png)  \n",
      "#### 결과물  \n",
      "-   Golden Gate assembly를 위한 vector assembly mixture 2 종\n",
      "### \\[Liquid handling\\] Golden Gate assembly mixture 제작  \n",
      "#### 20240604  \n",
      "#### 시약  \n",
      "-   DNA parts (spacer), lycopene 들어간 vector (V6L, V7L)  \n",
      "-   DW  \n",
      "-   T4 DNA ligase (HC) (Promega)  \n",
      "-   BsaI restriction enzyme (NEB)  \n",
      "-   10x T4 DNA ligase buffer (Promega)  \n",
      "#### 소모품  \n",
      "-   Pipet tip (10p, 200p tip)  \n",
      "-   PCR tube  \n",
      "-   PCR tube rack  \n",
      "#### 장비  \n",
      "-   Pipet (10p, 2.5p, 200p)  \n",
      "-   freezer  \n",
      "#### 방법  \n",
      "-   대량 자동화 수행 시, Janus, Echo 525 를 위한 추가적인 프로토콜이 필요함\n",
      "-   농도가 측정된 파트들을 계산하여 10 nM (100 fmol/10 $\\mu l$ ) 이상이 되도록 함 (volume은 10 $\\mu l$ 로 맞춤)\n",
      "-   볼륨을 맞추기 위해 DW를 넣어줌\n",
      "-   10x ligase buffer와 ligase, restriction enzyme을 넣음\n",
      "-   많은 양을 제작할 때는 stock으로 만든 후 소분\n",
      "-   실험에 사용된 부품과 양은 Assembly_025.xlsx의 240604 sheet를 참고  \n",
      "![](images/paste-1.png)  \n",
      "#### 결과물  \n",
      "-   Golden Gate assembly를 위한 vector assembly mixture 2 종\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# query it\n",
    "query = \"Spacer_connection에서 Golden Gate assembly mixture 제작\"\n",
    "doc1 = vectordb.similarity_search(query)\n",
    "## show top two \n",
    "print(\"====================================\")\n",
    "print(len(doc1))\n",
    "print(doc1[0].page_content)\n",
    "print(doc1[1].page_content)\n",
    "\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 10})\n",
    "doc2 = retriever.invoke(query)\n",
    "print(\"====================================\")\n",
    "print(len(doc2))\n",
    "print(doc2[0].page_content)\n",
    "print(doc2[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77c01283-d72c-46d7-b2c6-7e0ca61caf11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1c05467eb38493797222ba0938b5187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting the `device` argument to None from 0 to avoid the error caused by attempting to move the model that was already loaded on the GPU using the Accelerate module to the same or another device.\n"
     ]
    }
   ],
   "source": [
    "# huggingface model load\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "hf_evee = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"yanolja/EEVE-Korean-Instruct-10.8B-v1.0\",  # 사용할 모델의 ID를 지정합니다.\n",
    "    task=\"text-generation\",  # 수행할 작업을 설정합니다. 여기서는 텍스트 생성입니다.\n",
    "    # 사용할 GPU 디바이스 번호를 지정합니다. \"auto\"로 설정하면 accelerate 라이브러리를 사용합니다.\n",
    "    device=0,\n",
    "    # 파이프라인에 전달할 추가 인자를 설정합니다. 여기서는 생성할 최대 토큰 수를 10으로 제한합니다.\n",
    "    pipeline_kwargs=dict(max_new_tokens= 2048,\n",
    "                         repetition_penalty=1.2,),\n",
    "    model_kwargs={\"quantization_config\": quantization_config,\n",
    "                  \"temperature\": 0.2},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b797cf0a-b5ff-44bb-8c34-efc95a386265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please choose an model:\n",
      "1. Solar model\n",
      "2. llama-3-Korean-Bllossom-8B\n",
      "3. gpt-4o\n",
      "4. EEVE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================================\n",
      "\n",
      "client=<openai.resources.chat.completions.Completions object at 0x7ff7aeee0b50> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7ff7a46e5d80> model_name='gpt-4o' openai_api_key=SecretStr('**********') openai_proxy=''\n",
      "\n",
      "===================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response:\n",
      " ## Multi-module to plasmid (gibson assembly)\n",
      "\n",
      "### [Thermocycling] Thermocycler를 이용한 Gibson assembly 진행\n",
      "\n",
      "#### 시약\n",
      "- Gibson assembly를 위해 제작한 module mixture 종\n",
      "\n",
      "#### 장비\n",
      "- Thermocycler (Bio-rad)\n",
      "\n",
      "#### 방법\n",
      "- part mixture를 홈에 맞춰 Thermocycler에 넣음\n",
      "- 뚜껑을 닫고 조임\n",
      "- Gibson assembly 조건에 맞추어 작동\n",
      "\n",
      "| Steps | temperature | time  | description      |\n",
      "|-------|-------------|-------|------------------|\n",
      "| 1     | 37℃         | 50min | initial reaction |\n",
      "| 2     | 50℃         | 5h    | reaction         |\n",
      "| 3     | 4℃          | \\~    |                  |\n",
      "\n",
      "- Gibson assembly 반응이 끝난 뒤 샘플을 냉동고에 보관\n",
      "\n",
      "#### 결과물\n",
      "- MVA pathway가 모두 들어간 gibson assembly product -10 μL PCR tube\n"
     ]
    }
   ],
   "source": [
    "def query_chroma(query_text, llm):\n",
    "\n",
    "    retriever = vectordb.as_retriever(\n",
    "        search_type='mmr',\n",
    "        search_kwargs={'k': 5, 'lambda_mult': 0.2}\n",
    "        )\n",
    "\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    # print(docs)\n",
    "    # print(\"----------\")\n",
    "    # question = \"how can I conduct golden gate assembly today?\"\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        PROMPT_TEMPLATE.format(system=SYSTEM_PROMPT, context='{context}', question='{question}')\n",
    "    )\n",
    "\n",
    "    format_docs = '\\n\\n'.join([d.page_content for d in docs])\n",
    "\n",
    "    # print(format_docs)\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    response = chain.invoke({'context': format_docs, 'question': query_text})\n",
    "\n",
    "    print(\"response:\\n\", response)\n",
    "    return response\n",
    "\n",
    "def llm_list():\n",
    "    options = [\n",
    "               (Ollama(model=\"solar:latest\"), \"Solar model\"), \n",
    "               (Ollama(model=\"llama-3-Korean-Bllossom-8B:latest\"), \"llama-3-Korean-Bllossom-8B\"),\n",
    "               (ChatOpenAI(model=\"gpt-4o\"), \"gpt-4o\"),\n",
    "               (hf_evee, \"EEVE\"),\n",
    "              ]\n",
    "    print(\"Please choose an model:\")\n",
    "    for i, (option, description) in enumerate(options, start=1):\n",
    "        print(f\"{i}. {description}\")\n",
    "    \n",
    "    choice = input(\"Enter the number of your choice: \")\n",
    "    print(\"\\n===================================================\\n\")\n",
    "    llm = options[int(choice) -1][0]\n",
    "    return llm\n",
    "\n",
    "def query_data():\n",
    "    llm = llm_list()\n",
    "    print(llm)\n",
    "    print(\"\\n===================================================\\n\")\n",
    "    user_input = input(\"Enter your question:\").lower()\n",
    "    query_text = user_input\n",
    "    query_chroma(query_text, llm)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ysla_venv",
   "language": "python",
   "name": "ysla_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
